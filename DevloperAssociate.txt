1. Serverless -Lambda:
	a) versions -- used to publish one or more versions of your Lambda function
	b) aliases -- used to create different versions for single lambda function for testing purpose(like pointers)
				--Users can access the function version using the alias ARN
	c) Layers -- used to store the library/dependency modules for functions
	d) EnvironmentVariables -- you can encrypt those, or you can use AWS system parameter store for sensitive data
	e) Events -- You invoke Lambda based on Certain Events from different AWS Services
	f) APIGateway -- used to invoke Lambda from outside of AWS (REST API), to the world
	g) Lambda Monitoring we can use -- cloudwath and X-ray
	h) Scheduled Events -- we can done by using Cloudwatch Events
	i) AutoPublishAlias will create new alias and publish new version of the lambda code to this reference
	g) Linear10PercentEvery10Minutes -- Every 10 min 10% of traffic is shifted
	h) Canary10Percent10Minutes -- 10% traffic shifted first and all traffic shifted in thereby
	i) Data Stream Consumer -- Maximize throughput and minimize latency, Dedicated connection.
	j) iteratorAge --Stream records processed by Lambda function.
	k) AWS Mobile SDK uses context object gives automatic access to device and application using Lambda
	l) use step functions to coordinate with flow of other AWS Lambda function
	m) Dead letter Queue is used to debug asynchronus invocations of lambda
	n) if Lambda wants to access within VPC resources, you must have subnet ID and Security Group ID same
	o) IAM Role assigned to the lambda function has access to X-ray service
	p) Errors like serviceException, best practise is to Retry Invoking Lambda function

application traffic shifted from one alias to another alias:
create ALIAS with the --routing config parameters
update ALIAS with the --routing config parameters

change in the lambda version referred by an S3 bucket:
specify the lambda ALIAS ARN in the notification configuration.

specify the version to be deployed in a AppSpec file in codeDeploy



2. DynamoDB:
READ:
	Eventual consisten reads -- 2RCU for 2(4KB) = 8KB
	Strong consistent reads -- 1 RCU for upto 4KB 
Write:
	1WCU for upto 1KB size, even 1.5KB rounded off to 2KB consumes 2WCU

	a) Partition key and sort key
	b) Read/Write capacity throughputs calculations
	c) Global secondary indexes, localsecondary indexes
	d) DAX for microseconds latency and in-Memory cache
	e) Query vs Scan and ProjectionExpression

	Query:(SQL using if,where,>=,<= conditional statements,sortkey, projected expressions)
	A query operation searches only primary key attribute values and supports a subset of comparison operators on key attribute values to refine the search process.

	Scan:(Entire table, consumes All RCU, SlowResponse)
	A scan operation scans the entire table. You can specify filters to apply to the results to refine the values returned to you, after the complete scan. (Max scan size return is 1MB)

	-- Accelerators (DAX -inMomery Cache for dynamoDB for from milli sec to microseconds query latency)

	-- Global Tables -- automatic multi-master replication to AWS Regions worldwide (decrease Latency)

	-- Streams -- all changes (puts, updates, and deletes) are tracked on a rolling 24-hour basis.
				-- streams integrate with Lambda and invoke based on events in streams
				-- we can manipulate data before inserting into the dynamoDB with lambda

	-- triggers (Streams + Lambda == DynamoDB Triggers) (NO default Triggers in DynamoDB)

	-- A global secondary index (EVENTUAL CONSISTENCY ONLY)lets you QUERY over the entire table, across all 		partitions. and we can create addtional partition key /sort key at any time on table.

	-- RCU/WCU provisioned seperately for GSI

	-- A local secondary index lets you QUERY over a single partition, as specified by the partition key value in 		the query. and only supports additional sort key to be created and cannot be created after table has been created.

	-- BatchGetItem API Allows you to pass mutliple Partition key values in single request.
	-- DynamoDB by default encryption is enabled on server side

	-- best practices to avoid scans:
		a) use parallel-scans
		b) use Queries


    Query: Query operation is performed when you want to filterout data using Primary/sort key.
	Scan: Scan operation will fetch the entire table for you.

	To overcome ProvisionedThroughPutExceedException use:
		a) Exponential Backoff Algorithm
		b) AWS SDK --use jitter to automatically retries the failed request.

	Local Secondary Indexes only supports additional sort key and cannot be created after table has been created.

	ReturnConsumedCapacity in DyanmoDB table [None, Total, Index(gives aggregate consumption)]

3. KMS:
	a) AWS KMS vs CMK generated by CloudHSM(Hardware Security Module)
	b) symmetric and asymetric keys
	c) masterkey vs datakey
	d) rules are evaluted in sequential order, IAM role for first matching rule is used, unless a customRole ARN is specified.
	with DynamoDB:
	AWS Owned CMK -- Free
	AWS Managed CMK -- Cost
	Customer Managed CMK are not supported with encryption at rest. 
	DynamoDB uses CMK to generate unique key known as table key
	with encryption at rest, we can include primary and global secondary indexes


4. cloudwatch:
	a) LogMonitoring, Analysing, Dashboards
	b) LambdaEvents, Cronjobs, schedules
	c) Alarams
	d) Cloudtrail and CloudWatch Events are the proper tools to monitor CodePipeline.
	e) By default 1 minute metrics available, if you want 10 sec metrics create high resolution metircs.
	f) AWS X-ray -- detailed tracking of latency and analysis of request.
	g) On-Premise instances do not use IAM instance Profiles.
	h) AWS X-ray -- 1 req/sec + 5% any additional req/host


5. cloudformation:
	a) StckCreation template
	b) Optional: templateValues, outputs, dataTables
	   required: Resources, file format version number
	c) take params at run times using parametrs section from AWS Secret Manager.
	d) StackSet allows to create stacks in mutlple regions
	e) parameter section used to pass values dynamically in cloudfromation template
	f) preconfigurations scripts in cloudformation can be put in cfn-init helper script



6. ElasticBeanstalk:
	a) PaaS service
	b) immutable and blue/green deployment allows high efficiency deployments
	c) EB Launch configuration and environment manifestfile to change cofiguration 


7. CodeSuite --- Devlopers, code, build and deploy (CI/CD):

	code deployemt fails with the HELATH_CONTRAINTS_INVALID:
	Reduce the no.of helathy instance required during the deployment.

	CLI command with buildSpecOverride property set to new buildspec.yaml file in codebuild
	If you remove an instance from a deployment group, CodeDeploy does not uninstall anything that might have already been installed on that instance.

8. S3:

	Storage Limits:
		Indivudaul object Range --- 0 bytes to 5 terbytes
		objects >100 MB Should use Multipart upload capability.
		Largest object in single PUT is 5 GB
		Maximum 100 buckets per Account

	Storage Clases:
		S3 Standard
		S3 Standard-IA
		S3 One Zone-IA -- for long lived but infrequest access data
		S3 Glacier and S3 Glacier Deep Archive -- for long living data

	Billing:
		pay as you go

	Security:
		Bucket policies and ACL Policies
		controlAcess through:
		1. IAM policies -- user access what he can do in AWS Account on particular resource
		2. Buckets policies -- define rules to all buckets to access or restrict (Ex: Http/Https or based on IP)
		3. Access Control List (ACL) policies -- (READ, WRITE, FULL_CONTROL) per user per bucket/object
		4. Query String Authentication -- customers can create a URL to an Amazon S3 object which is only valid for a limited time
	

	Query in place:
		-- run sophisticated queries against data stored without the need to move data into a separate analytics platform. the following are used
		a) S3 select (we can SQL Clauses to query like SELECT, WHERE)
		b) Amazon Athena (Serverless architecture same to query data directly from S3)
		c) Amazon RedShiftSpectrum (Run queries against unstructured data)

	
	S3 Transfer Acceleration: (uses cloudfront in backend)
		-- designed to optimize transfer speeds from across the world into S3 buckets
		-- must use these two endpoints s3-accelerate.amazonaws.com or .s3-accelerate.dualstack.amazonaws.com 

	Snow Family (Snowball, Snowball Edge, and Snowmobile):
		-- useful when large batches of data at once take 5-7 days turnaround time

	S3 Inventory:
		-- it's a scheduled alternative to Amazon S3â€™s synchronous List API.
		-- You can simplify and speed up business workflows and big data jobs or replication/encryption with S3 Inventory.
		-- to findout which object has millions of versions avilable in the bucket.

	S3 Batch Operations:
		-- automate the execution, management, and auditing of a specific S3 request or Lambda function
		-- a) automate replacing tag sets on S3 objects
		-- b) updating access control lists (ACL) for S3 objects
		-- c) copying storage between buckets
		-- d) initiating a restore from Glacier to S3
		-- e) performing custom operations with Lambda functions

	object Lock:
		--S3 Object Lock prevents deletion of an object for the duration of a specified retention period
		--make an object immutable by applying a Legal Hold to that object


	LifecyclePolicy: (Helps to reduce storage cost)
		-- automatically remove objects based on the age of the object.
		--S3 Lifecycle management provides the ability to define the lifecycle of your object
		--You can set a lifecycle transition policy to automatically migrate objects stored in the S3 storage classes

	S3 Replication: (Decrease the latancy)
		-- enables automatic, asynchronous copying of objects across Amazon S3 buckets.
		-- with SRR(same region replication) and CRR(cross region replication) or different AWS Accounts

	S3 Data pipeline -- Automate the moment and transformation of the data.

	use suffixes for better partitioning the data
	Only 100 buckets are allowed per AWS Account


9. SQS: (distributed and decoupled systems):
	
	a) Standard Queue:(Scalable and highly distributed)
		-- Delivery at least once
		-- NO Guranteed for order of Messages
	b) FIFO Queue:
		-- Delivery exactly Once, No Duplicates
	 	-- guranteed order of delivery

	c) Polling:
		-- Long polling --waits until message arrive in the queue (Recommended, reduces cost)
		-- Short polling -- immediately return even if attributes returns empty

	d) Limits:
		-- message size 1KB to max of 256 KB
		-- retention period 1 minute to 14 days
		-- visibility timeout min 0 sec to default 30 sec and max of 12 hours

	e) Queue Sharing:
		-- you can share queue with another AWS account


	SQS Long polling used for reducing the empty queries as well as cost.
	SQS visibility timeout makes the message invisible after the message has been read not in the begining.
	Delay queue is used for this purpose or you can use message timer attribute for each message.

	if you want to send message upto 456KB in size SQS, use Exended Client library
	NO limit for the no.of message Queues in AWS account
	ReceiveMessageWaitTimeSeconds attribute to 20 seconds to reduce the no.of empty queries. (you can use long poll also).

	You can Acess Queue Anonymously.

10. SNS(Notification service):

	Fanout:
	A message published to SNS topic distibuted to a no.of SQS Queues in parallel, asynchronous processing.

	SubjectID is not avaialble in the SNS message body

	Name,type and value must not be empty or null, in addition  message body shouldn't empty or null from SQS 

	SNS Supports Email, Json, SMS, HTTP/HTTPS

	SNS can deliver upto 10 message attributes from SQS

11. API Gateway
	a) fully managed service
	b) integration request/response to communicate backend
	e) method request/service to communication frontend
	f) Authorizers enable you to control access to your APIs using Amazon Cognito User Pools or a Lambda function.
	g) cognito and Resource based policies supports for external authentication.

12. Elastic Cache:(popular choice for Gaming, Ad-Tec, finanacial services,Healthcare and IoT apps)
    a) Redis
    b) memcached
	
	Redis features:
	-- Automatic detection and Recovery from cache node failures
	-- Multi A-Z with automatic failover, create Read replica.
	-- Redis(Cluster mode enabled) supports partitioning data accross upto 15 shards
	-- in-transit and data at-rest encryption with authentication (HIPPA Compliant)
	-- Flexible Availability Zone placement of Nodes and cluster for increased fault tolerance.
	-- data structure server not only cachie but also DB
		a) shared Queue
		b) message solution
		c) sessions and sorted sets
		d) 512MB/key

	-- Redis Lazy Loading -- only requested data is cached.
	-- Redis Write through -- always updates cache content for every request.
	-- Redis TTL -- retention period of the Cache item in memory.

	b) MemCached features:
	-- simple key-value Cache.(1MB/key)
	-- Memcached is faster compared to Redis.
	-- Native clustering is availble. Hence Robust mechanism achieved.
	-- cache size is 1MB/key ONLY
	-- Great for multithreaded environment



13. AWS Cognito:
	a) 	--Mobile and web application Authentications
	   	-- supports 
			a) SAML(SSO Single-sign-on)
			b) OpenID connect(OAuth Extension), 
			c) Social Identity Providers(fb,amazon,twitter)
		-- CognitoSync data across a userâ€™s devices/platforms/applications.when they switch between devices or upgrade.
		-- offlined data syncronization to server when network available.
		-- verify email, phone, MFA, password policies, login using phone, email all are supported.
		-- customize singin/singup work flows

	b) user pool:
		-- securley store the users profile attributes.
	c) Identity pool:
		-- conatins federated identities list, but not user profile or attributes
	d) support both for Authenticated user and Guest users
	e) CognitoStreams -- Sync data store to kinesis stream, from there to RDS, Redshift, even S3 file.
	f) CognitoEvents -- allows developers to run an AWS Lambda function in response to important events in Cognito

14. Load balancer, EC2, AutoScaling:

	EBS Encryption -- Ensure that the encryption is enabled during volume creation time.

	Automatic configuration of tasks and even run scripts after instance starts. you can use user data of two types
	shell-script and cloud-init directives

	context keys and iam simulate-custom-policy-command to test policy permissions via CLI

	Kinesis Data Firehose Streaming Encryption at rest:

		a) Enable encryption on the firehose stream
		B) streams are used to transfer the data form the producers

	
15. AMI & EC2:

	a) DescribeImages API
	b) EBS-Backed instance can be stopped and started.
	c) AWS --> VPC --> Subnets --> RouteTable --> InternetGateway
	d) In 1 subnet, you must have only 1 routetable
	e) VPN --> connect your VPC to remote networks/office networks
	VPC are not accessible by AWS codeBuild. if you want to provide access, must give additional VPC-specific configuration.
		a) VPC ID
		b) VPC Subnet ID
		c) VPC Security Group ID
		d) VPC enabled builds
	when ECS instance is stop, container status is ACTIVE coantainer agent status changes to FALSE immediately
	f) Resource policy -- access in between AWS managed services use resource policies.
	g) IAM policy -- providing access from some user/external system/user managed instances

16. Kinesis Data Streams:
	Accelerated log and data feed intake
	Real-time metrics and reporting
	Real-time data analytics
	Complex stream processing
	user existing or new stream to analyze cognito data using redshift service
	kinesis data Firehouse: delivering realtime streaming data to destinations such as S3, redshift, elasticsearch
	Envelope Encryption:
	data is encrypted using the plaintext data key, the data key is then further encrypted using a plainText master key
	this plainText master key is further securley stored in AWS KMS  and CMS

17. IAM & Policies:
	AWS Policy main parameters:
	a) Sid
	b) Effect
	c) Principal
	d) Action
	e) Resource
	f) Condition
	STS AssumeRole is used to IAM role permissions to access S3 bucket (first test 8th question)
	AWS Secret Manager to store database credentials safely (Only store encrypted credentials)
	AWS System Parameter Store the credentials (both plaintext and Encrypted format)
	PolicyEvaluationLogic-- IAM STSAssumeRole (Deny by default)




ECS -- Orchestration service, Docker service in AWS
AWS Redshift --- data warehouse data needs to be ported(giga bytes to peta bytes)
SQS -- Decoupling or Distributive service in AWS
OpsWorks --  lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances
Kinesis:
	Data Streams -- Collect streaming data, at scale, for real-time analytics (IT logs, location-tracking, social media etc)
	Data Firehouse -- delivering realtime streaming data to destinations such as S3, Redshift, elasticsearch
	Data Analytics -- used to process and analyze streaming data using standard SQL. create dashboards and metrics in 							realtime.

# To call each lambda version for testing purpose from .NET program, we can create alias and reference it in the program
# On-premise instances do not use IAM Insatance profiles
On-Premise instance:
	any physical device that is not an Amazon EC2 instance (May be localhost, physical server setup,server outside of AWS) that can run the CodeDeploy agent and connect to public AWS service endpoints


Interceptors to add your code to trace incoming HTTP Requests.
Create a deployment package with your code and third party libraries.(like graphic library for image processing)
Object to hold in cloudfront:
	Minimum TTL in cloudfront cache behavior
	configure the origin to add an Expires header field to the object.
Encryption process locally:
	use the Operation (GenerateDatakey) to get a data encryption key

	use the plaintext data encryption key to encrypt data locally. then erase the plaintext data key form memory

	store the encrypted data key along side the locally encrypted data.

DynamoDB streams enable time order sequence of item-level modification in the table.

HTTP 400 status code in DynamoDB 
	a) missing requiered parameters
	b) Exceeds the table provisioned throughput.


Specify a key condition expression in the query
Partition key name and value in the equality conditions


S3 prefixes and Backoff algroithms